{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "#Import all the required Packages and Libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer as cv\n",
    "import pyspark.sql.functions as f\n",
    "import numpy as np\n",
    "import pandas\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from pandas import DataFrame\n",
    "import csv\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Phone Book - Country Look up\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\")\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set('spark.sql.caseSensitive', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv file\n",
    "df = ps.read_csv('reviews.csv')\n",
    "\n",
    "#Conver Score to Integer\n",
    "df['Score'] = df['Score'].astype('int64')\n",
    "\n",
    "#Get the mean of the Score\n",
    "mean = df['Score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int32 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568454 non-null  object\n",
      " 4   HelpfulnessNumerator    568452 non-null  object\n",
      " 5   HelpfulnessDenominator  568452 non-null  object\n",
      " 6   Score                   568162 non-null  int64 \n",
      " 7   Time                    568449 non-null  object\n",
      " 8   Summary                 568448 non-null  object\n",
      " 9   Text                    568444 non-null  object\n",
      "dtypes: int32(1), int64(1), object(8)None\n",
      "count    568162.000000\n",
      "mean          4.176305\n",
      "std           1.383878\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%           5.000000\n",
      "75%           5.000000\n",
      "max          69.000000\n",
      "Name: Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Basic Information about the dataset\n",
    "print(df.info())\n",
    "print(df[\"Score\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Null Values with the mean of the score and rest with \"No\"\n",
    "df.fillna({'Summary': 'No Summary'}, inplace=True)\n",
    "df.fillna({'Text': 'No Text'}, inplace=True)\n",
    "df.fillna({'ProfileName': 'No ProfileName'}, inplace=True)\n",
    "df.fillna({'HelpfulnessNumerator': 'No HelpfulnessNumerator'}, inplace=True)\n",
    "df.fillna({'HelpfulnessDenominator': 'No HelpfulnessDenominator'}, inplace=True)\n",
    "df.fillna({'Time': 'No Time'}, inplace=True)\n",
    "df.fillna({'Id': 'No Id'}, inplace=True)\n",
    "df.fillna({'ProductId': 'No ProductId'}, inplace=True)\n",
    "df.fillna({'UserId': 'No UserId'}, inplace=True)\n",
    "df.fillna({'Score': mean}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   Id                      568454 non-null  object \n",
      " 1   ProductId               568454 non-null  object \n",
      " 2   UserId                  568454 non-null  object \n",
      " 3   ProfileName             568454 non-null  object \n",
      " 4   HelpfulnessNumerator    568454 non-null  object \n",
      " 5   HelpfulnessDenominator  568454 non-null  object \n",
      " 6   Score                   568454 non-null  float64\n",
      " 7   Time                    568454 non-null  object \n",
      " 8   Summary                 568454 non-null  object \n",
      " 9   Text                    568454 non-null  object \n",
      "dtypes: float64(1), object(9)None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the count and mean value as group by the products\n",
    "count = df.groupby(\"UserId\", as_index=False).count()\n",
    "mean = df.groupby(\"UserId\", as_index=False).mean()\n",
    "\n",
    "#merge two dataset create df1\n",
    "df1 = ps.merge(df, count, how='right', on=['UserId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "df1[\"Count\"] = df1[\"ProductId_y\"]\n",
    "df1[\"Score\"] = df1[\"Score_x\"]\n",
    "df1[\"Summary\"] = df1[\"Summary_x\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New datafram with selected variables\n",
    "df1 = df1[['UserId','Summary','Score',\"Count\"]]\n",
    "df1 = df1.sort_values('Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.pandas.frame.DataFrame'>\n",
      "Int64Index: 393063 entries, 544569 to 568443\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   UserId   393063 non-null  object \n",
      " 1   Summary  393063 non-null  object \n",
      " 2   Score    393063 non-null  float64\n",
      " 3   Count    393063 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(2)None\n",
      "count    393063.000000\n",
      "mean          4.185372\n",
      "std           1.371463\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%           5.000000\n",
      "75%           5.000000\n",
      "max          69.000000\n",
      "Name: Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Get the reviews more than or equal to 2\n",
    "df2 = df1[df1.Count >= 2]\n",
    "#Insights from the dataset2 whohave count more than 2\n",
    "print(df2.info())\n",
    "print(df2[\"Score\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe as combining all summary with same product Id\n",
    "df4 = df.groupby(\"UserId\", as_index=False).mean()\n",
    "combine_summary = df2.groupby(\"UserId\")[\"Summary\"].apply(list)\n",
    "combine_summary = ps.DataFrame(combine_summary)\n",
    "#Store the intermediate result for my reference\n",
    "combine_summary.to_excel(\"combine_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create with certain columns\n",
    "df3 = ps.read_excel(\"combine_summary.xlsx\")\n",
    "df3 = ps.merge(df3, df4, on=\"UserId\", how='inner')\n",
    "#Store the intermediate result for my reference\n",
    "df3.to_excel(\"uncleaned_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "pdf=pandas.read_excel(\"uncleaned_summary.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data and use nltk to tokenize the words\n",
    "cleanup_re = re.compile('[^a-z]+')\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = cleanup_re.sub(' ', sentence).strip()\n",
    "    sentence = \" \".join(nltk.word_tokenize(sentence))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the cleanup function to the data\n",
    "pdf['Summary_Clean'] = pdf['Summary'].apply(cleanup)\n",
    "pdf=pdf.drop_duplicates(['Score'], keep='last')\n",
    "pdf=pdf.reset_index()\n",
    "df=spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|index|Unnamed: 0|        UserId|             Summary|            Score|       Summary_Clean|               words|\n",
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+\n",
      "|   28|        28|A106ZCP7RSXMRU|['Natural organic...|4.716666666666667|natural organic a...|[['natural, organ...|\n",
      "|  364|       364|A14RZUPW44KCPF|['Crack for Cats!...|4.655172413793103|crack for cats st...|[['crack, for, ca...|\n",
      "|  369|       369|A14TUWXDA5WQ7W|['1268179200', '1...|             6.75|                    |[['1268179200',, ...|\n",
      "|  604|       604|A185QFJRTB5W93|['Great Breading'...|4.527777777777778|great breading fo...|[['great, breadin...|\n",
      "|  763|       763|A1AEPMPA12GUJ7|['4', '4', '4', '...|0.391304347826087|                    |[['4',, '4',, '4'...|\n",
      "| 1194|      1194|A1GQAKL9CGQLP1|['Great Dark Choc...|4.955223880597015|great dark chocol...|[['great, dark, c...|\n",
      "| 1226|      1226|A1H703P9ALYVM6|['Too Mild for Me...|3.138888888888889|too mild for me g...|[['too, mild, for...|\n",
      "| 1239|      1239|A1HG82U7GY9531|['Sweet Love = Gr...|4.310344827586207|sweet love great ...|[['sweet, love, =...|\n",
      "| 1813|      1813|A1PI8VBCXXSGC7|['Great tasting M...|4.453333333333333|great tasting mil...|[['great, tasting...|\n",
      "| 2054|      2054|A1T61QP7QHYBRQ|['Sweet and Sooth...|4.738095238095238|sweet and soothin...|[['sweet, and, so...|\n",
      "| 2752|      2752|A22JHOEDZED75E|['Popchips are he...|4.060606060606061|popchips are heal...|[['popchips, are,...|\n",
      "| 2812|      2812|A23E9ZPS2RQZS0|['Finicky cats', ...|4.146341463414634|finicky cats exce...|[['finicky, cats'...|\n",
      "| 2986|      2986|A25VFHVGI4CFTP|['Warning!  WARNI...| 1.19047619047619|warning warning a...|[['warning!, , wa...|\n",
      "| 3294|      3294|A29V32IOAJUP8P|['Great product, ...|4.617647058823529|great product ver...|[['great, product...|\n",
      "| 3351|      3351|A2AOD7254MCKSH|['the only salt I...|             4.62|the only salt i u...|[['the, only, sal...|\n",
      "| 3375|      3375|A2B5OI74EHGVH1|['dripping in oil...|1.433333333333333|dripping in oil d...|[['dripping, in, ...|\n",
      "| 3508|      3508|A2D1LPEUCTNT8X|[\"Love it! I use ...| 4.47191011235955|love it i use it ...|[[\"love, it!, i, ...|\n",
      "| 3596|      3596|A2E2PA6UNK1E05|['Good Taste but ...| 4.17948717948718|good taste but no...|[['good, taste, b...|\n",
      "| 3779|      3779|A2GLD72HQYHG0P|['Best Grocery St...| 4.54054054054054|best grocery stor...|[['best, grocery,...|\n",
      "| 3865|      3865|A2HQ8RTAJYEUJO|['3.5* snack for ...|3.689655172413793|snack for a non c...|[['3.5*, snack, f...|\n",
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create theTokenizer and transform the data\n",
    "tokenizer=Tokenizer(inputCol=\"Summary\",outputCol=\"words\")\n",
    "words=tokenizer.transform(df)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|index|Unnamed: 0|        UserId|             Summary|            Score|       Summary_Clean|               words|         rawFeatures|\n",
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|   28|        28|A106ZCP7RSXMRU|['Natural organic...|4.716666666666667|natural organic a...|[['natural, organ...|(22167,[0,1,2,3,4...|\n",
      "|  364|       364|A14RZUPW44KCPF|['Crack for Cats!...|4.655172413793103|crack for cats st...|[['crack, for, ca...|(22167,[3,10,29,3...|\n",
      "|  369|       369|A14TUWXDA5WQ7W|['1268179200', '1...|             6.75|                    |[['1268179200',, ...|(22167,[1591,4670...|\n",
      "|  604|       604|A185QFJRTB5W93|['Great Breading'...|4.527777777777778|great breading fo...|[['great, breadin...|(22167,[0,1,3,5,6...|\n",
      "|  763|       763|A1AEPMPA12GUJ7|['4', '4', '4', '...|0.391304347826087|                    |[['4',, '4',, '4'...|(22167,[219,220,5...|\n",
      "| 1194|      1194|A1GQAKL9CGQLP1|['Great Dark Choc...|4.955223880597015|great dark chocol...|[['great, dark, c...|(22167,[0,1,2,3,5...|\n",
      "| 1226|      1226|A1H703P9ALYVM6|['Too Mild for Me...|3.138888888888889|too mild for me g...|[['too, mild, for...|(22167,[0,2,3,4,1...|\n",
      "| 1239|      1239|A1HG82U7GY9531|['Sweet Love = Gr...|4.310344827586207|sweet love great ...|[['sweet, love, =...|(22167,[0,1,2,3,4...|\n",
      "| 1813|      1813|A1PI8VBCXXSGC7|['Great tasting M...|4.453333333333333|great tasting mil...|[['great, tasting...|(22167,[0,1,2,3,4...|\n",
      "| 2054|      2054|A1T61QP7QHYBRQ|['Sweet and Sooth...|4.738095238095238|sweet and soothin...|[['sweet, and, so...|(22167,[0,3,4,6,8...|\n",
      "| 2752|      2752|A22JHOEDZED75E|['Popchips are he...|4.060606060606061|popchips are heal...|[['popchips, are,...|(22167,[0,1,3,4,1...|\n",
      "| 2812|      2812|A23E9ZPS2RQZS0|['Finicky cats', ...|4.146341463414634|finicky cats exce...|[['finicky, cats'...|(22167,[0,1,3,5,6...|\n",
      "| 2986|      2986|A25VFHVGI4CFTP|['Warning!  WARNI...| 1.19047619047619|warning warning a...|[['warning!, , wa...|(22167,[3,9,15,20...|\n",
      "| 3294|      3294|A29V32IOAJUP8P|['Great product, ...|4.617647058823529|great product ver...|[['great, product...|(22167,[6,21,24,2...|\n",
      "| 3351|      3351|A2AOD7254MCKSH|['the only salt I...|             4.62|the only salt i u...|[['the, only, sal...|(22167,[0,1,2,3,4...|\n",
      "| 3375|      3375|A2B5OI74EHGVH1|['dripping in oil...|1.433333333333333|dripping in oil d...|[['dripping, in, ...|(22167,[2,4,8,9,1...|\n",
      "| 3508|      3508|A2D1LPEUCTNT8X|[\"Love it! I use ...| 4.47191011235955|love it i use it ...|[[\"love, it!, i, ...|(22167,[0,1,2,3,4...|\n",
      "| 3596|      3596|A2E2PA6UNK1E05|['Good Taste but ...| 4.17948717948718|good taste but no...|[['good, taste, b...|(22167,[0,1,2,3,4...|\n",
      "| 3779|      3779|A2GLD72HQYHG0P|['Best Grocery St...| 4.54054054054054|best grocery stor...|[['best, grocery,...|(22167,[0,1,2,3,4...|\n",
      "| 3865|      3865|A2HQ8RTAJYEUJO|['3.5* snack for ...|3.689655172413793|snack for a non c...|[['3.5*, snack, f...|(22167,[1,2,3,4,5...|\n",
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the CountVectorizer to create the word count by fiting and transforming the data\n",
    "count=CountVectorizer(inputCol=\"words\",outputCol=\"rawFeatures\")\n",
    "model=count.fit(words)\n",
    "result=model.transform(words)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|Unnamed: 0|        UserId|             Summary|            Score|       Summary_Clean|               words|         rawFeatures|            features|\n",
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   28|        28|A106ZCP7RSXMRU|['Natural organic...|4.716666666666667|natural organic a...|[['natural, organ...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "|  364|       364|A14RZUPW44KCPF|['Crack for Cats!...|4.655172413793103|crack for cats st...|[['crack, for, ca...|(22167,[3,10,29,3...|(22167,[3,10,29,3...|\n",
      "|  369|       369|A14TUWXDA5WQ7W|['1268179200', '1...|             6.75|                    |[['1268179200',, ...|(22167,[1591,4670...|(22167,[1591,4670...|\n",
      "|  604|       604|A185QFJRTB5W93|['Great Breading'...|4.527777777777778|great breading fo...|[['great, breadin...|(22167,[0,1,3,5,6...|(22167,[0,1,3,5,6...|\n",
      "|  763|       763|A1AEPMPA12GUJ7|['4', '4', '4', '...|0.391304347826087|                    |[['4',, '4',, '4'...|(22167,[219,220,5...|(22167,[219,220,5...|\n",
      "| 1194|      1194|A1GQAKL9CGQLP1|['Great Dark Choc...|4.955223880597015|great dark chocol...|[['great, dark, c...|(22167,[0,1,2,3,5...|(22167,[0,1,2,3,5...|\n",
      "| 1226|      1226|A1H703P9ALYVM6|['Too Mild for Me...|3.138888888888889|too mild for me g...|[['too, mild, for...|(22167,[0,2,3,4,1...|(22167,[0,2,3,4,1...|\n",
      "| 1239|      1239|A1HG82U7GY9531|['Sweet Love = Gr...|4.310344827586207|sweet love great ...|[['sweet, love, =...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "| 1813|      1813|A1PI8VBCXXSGC7|['Great tasting M...|4.453333333333333|great tasting mil...|[['great, tasting...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "| 2054|      2054|A1T61QP7QHYBRQ|['Sweet and Sooth...|4.738095238095238|sweet and soothin...|[['sweet, and, so...|(22167,[0,3,4,6,8...|(22167,[0,3,4,6,8...|\n",
      "| 2752|      2752|A22JHOEDZED75E|['Popchips are he...|4.060606060606061|popchips are heal...|[['popchips, are,...|(22167,[0,1,3,4,1...|(22167,[0,1,3,4,1...|\n",
      "| 2812|      2812|A23E9ZPS2RQZS0|['Finicky cats', ...|4.146341463414634|finicky cats exce...|[['finicky, cats'...|(22167,[0,1,3,5,6...|(22167,[0,1,3,5,6...|\n",
      "| 2986|      2986|A25VFHVGI4CFTP|['Warning!  WARNI...| 1.19047619047619|warning warning a...|[['warning!, , wa...|(22167,[3,9,15,20...|(22167,[3,9,15,20...|\n",
      "| 3294|      3294|A29V32IOAJUP8P|['Great product, ...|4.617647058823529|great product ver...|[['great, product...|(22167,[6,21,24,2...|(22167,[6,21,24,2...|\n",
      "| 3351|      3351|A2AOD7254MCKSH|['the only salt I...|             4.62|the only salt i u...|[['the, only, sal...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "| 3375|      3375|A2B5OI74EHGVH1|['dripping in oil...|1.433333333333333|dripping in oil d...|[['dripping, in, ...|(22167,[2,4,8,9,1...|(22167,[2,4,8,9,1...|\n",
      "| 3508|      3508|A2D1LPEUCTNT8X|[\"Love it! I use ...| 4.47191011235955|love it i use it ...|[[\"love, it!, i, ...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "| 3596|      3596|A2E2PA6UNK1E05|['Good Taste but ...| 4.17948717948718|good taste but no...|[['good, taste, b...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "| 3779|      3779|A2GLD72HQYHG0P|['Best Grocery St...| 4.54054054054054|best grocery stor...|[['best, grocery,...|(22167,[0,1,2,3,4...|(22167,[0,1,2,3,4...|\n",
      "| 3865|      3865|A2HQ8RTAJYEUJO|['3.5* snack for ...|3.689655172413793|snack for a non c...|[['3.5*, snack, f...|(22167,[1,2,3,4,5...|(22167,[1,2,3,4,5...|\n",
      "+-----+----------+--------------+--------------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using the IDF to create the TF-IDF by fiting and transforming the data\n",
    "idf=IDF(inputCol=\"rawFeatures\",outputCol=\"features\")\n",
    "idfModel=idf.fit(result)\n",
    "rescaledData=idfModel.transform(result)\n",
    "rescaledData.show()\n",
    "rescaledData=ps.DataFrame(rescaledData)\n",
    "#Store the result in the excel file for future use\n",
    "rescaledData.to_excel(\"summary_features.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Words and their frequencies\n",
    "docs=pdf['Summary_Clean']\n",
    "vect=cv(max_features=100, stop_words='english')\n",
    "X=vect.fit_transform(docs)\n",
    "\n",
    "\n",
    "#Exporting Countvectorizer data to pandas dataframe\n",
    "df1 = DataFrame(X.A, columns=vect.get_feature_names())\n",
    "df1=df1.astype(int)\n",
    "#Exporting in Excel\n",
    "df1.to_excel(\"WordScores.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USer Based Recommendation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a own model using euclidean distance\n",
    "def kNearestNeighborClassifier(pdataset,point,k):\n",
    "  results = {}\n",
    "  for point_item in pdataset:  \n",
    "    ecludian_distance=math.sqrt(np.sum(np.subtract(point_item,point)*np.subtract(point_item,point)))\n",
    "    if len(results)<k:\n",
    "      results[ecludian_distance] = point_item\n",
    "    else:\n",
    "      for max_key in sorted(results.keys(),reverse=True):\n",
    "              if(max_key>=ecludian_distance):\n",
    "                results[ecludian_distance]=point_item\n",
    "                results.pop(max_key)\n",
    "              break\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Create a own model using Cosine Similarity\n",
    "def kNNCosine(pdataset,point,k):\n",
    "    results = {}\n",
    "    for point_item in pdataset:  \n",
    "        cosine_distance=np.dot(point_item,point)/(norm(point_item)*norm(point))\n",
    "    if len(results)<k:\n",
    "      results[cosine_distance] = point_item\n",
    "    else:\n",
    "      for max_key in sorted(results.keys(),reverse=True):\n",
    "              if(max_key>=cosine_distance):\n",
    "                results[cosine_distance]=point_item\n",
    "                results.pop(max_key)\n",
    "              break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295743\n",
      "1097\n"
     ]
    }
   ],
   "source": [
    "#Read the data\n",
    "pdf=pandas.read_excel(\"WordScores.xlsx\")\n",
    "raw=pandas.read_excel(\"summary_features.xlsx\")\n",
    "rev=pandas.read_csv(\"Reviews.csv\")\n",
    "rev=rev.drop_duplicates(['Summary'], keep='last')\n",
    "rev[\"Score\"]=rev[\"Score\"].astype(int)\n",
    "rev=rev.reset_index()\n",
    "\n",
    "pdflen=len(pdf)\n",
    "revlen=len(rev)\n",
    "print(revlen)\n",
    "print(pdflen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a CSV file and add the headers in append mode\n",
    "with open('User_Recommendation.csv', 'a') as file:\n",
    "    writerObj = csv.writer(file)\n",
    "    writerObj.writerow(['User','NearestUser', 'Products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the model to the data Using Cosine Similarity  \n",
    "for i in range(pdflen):\n",
    "  point=pdf.iloc[i,0]\n",
    "  results=kNNCosine(pdf.iloc[i],point,11)\n",
    "  first_related_user=[item for item in results.values()] \n",
    "  #Initialize the list and append the related products to the list\n",
    "  products_list=[]\n",
    "  for j in first_related_user:\n",
    "     products_list.append(raw.iloc[j,3])\n",
    "  if len(products_list)>2:\n",
    "    products_list=products_list[:1]+products_list[2:]\n",
    "  products_list=products_list[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the model to the data using Euclidean Distance\n",
    "for i in range(pdflen):\n",
    "  point=pdf.iloc[i,0]\n",
    "  results=kNearestNeighborClassifier(pdf.iloc[i],point,11)\n",
    "  first_related_user=[item for item in results.values()]\n",
    "  #Initialize the list and append the related products to the list\n",
    "  products_list=[]\n",
    "  for j in first_related_user:\n",
    "     products_list.append(raw.iloc[j,3])\n",
    "  if len(products_list)>2:\n",
    "    products_list=products_list[:1]+products_list[2:]\n",
    "  products_list=products_list[0:2]\n",
    "  for k in range(revlen):\n",
    "    if (rev[\"UserId\"][k]==products_list[1] and rev[\"Score\"][k]>3):\n",
    "      products_list.append(rev[\"ProductId\"][k])\n",
    "  #Open the file in append mode and write the data to the file\n",
    "  with open('User_Recommendation.csv', 'a') as file:\n",
    "    writerObj = csv.writer(file)\n",
    "    writerObj.writerow(products_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check User_Recommendation.csv for the results of User based Recommendation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b8336780583f9fe5e92fb543d407fcf63bef7ccaace1ea6a7168a6d8bee36cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
